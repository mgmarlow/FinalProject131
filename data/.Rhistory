for (y in i:length(classes)) {
# check if in same cluster
if (classes[i] == classes[y]) {
# check if same class
if (clusters[i] != clusters[y]) {
# if same class, add lump error
count <- count + 1
}
}
}
}
return(count)
}
# test split_error function
split_error(c(1,2,2), c(2,1,1))
split_error(c(1,2,2), c(2,1,2))
kc1
str(kc1)
## Section A ##
x1 = c(1, 1, 0, 5, 6, 4)
x2 = c(4, 3, 4, 1, 2, 0)
obs = c(1, 2, 3, 4, 5, 6)
data = data.frame(x1, x2)
plot(x1, x2, main="Scatterplot of Observations")
## Section B ##
set.seed(1)
sample_obs = sample(obs) # 2 6 3 4 1 5
cluster <- c(2, 1, 1, 2, 2, 1)
data <- data.frame(data, cluster)
# cluster labels:
(cluster)
c1 <- data.frame(
x1 <- c(1, 0, 4),
x2 <- c(3, 4, 0)
)
(centroid1_x1 <- mean(c(data[2,]$x1, data[3,]$x1, data[6,]$x1)))
(centroid1_x2 <- mean(c(data[2,]$x2, data[3,]$x2, data[6,]$x2)))
mean()c1$x1
mean(c1$x1)
mean(c1$x2)
? col.p
plot(x1, x2, col.p=c('red', 'blue'))
?plot
plot(x1[1:3], x2[1:3])
par(new=T)
plot(x1[4:6], x2[4:6])
plot(x1[1:3], x2[1:3])
par(new=FALSE)
plot(x1[4:6], x2[4:6])
x1
x2
x1 = c(1, 1, 0, 5, 6, 4)
x2 = c(4, 3, 4, 1, 2, 0)
plot(x1[1:3], x2[1:3])
par(new=FALSE)
plot(x1[4:6], x2[4:6])
plot(x1[1:3], x2[1:3])
par(new=TRUE)
plot(x1[4:6], x2[4:6])
plot(x1[1:3], x2[1:3])
lines(x1[4:6], x2[4:6])
plot(x1,x2)
plot(x1,x2)
lines(x1[1:3], x2[1:3], col="red")
lines(x1[4:6], x2[4:6], col="blue")
plot(x1,x2, main="Plot of Observations with Clustering")
lines(x1[1:3], x2[1:3], col="red")
lines(x1[4:6], x2[4:6], col="blue")
? lines
? lgened
? legend
plot(x1,x2, main="Plot of Observations with Clustering")
lines(x1[1:3], x2[1:3], col="red")
lines(x1[4:6], x2[4:6], col="blue")
legend(0, 0, c('1', '2'))
legend(0, 50, c('1', '2'))
legend(0, 20, c('1', '2'))
legend(0, 200, c('1', '2'))
plot(x1,x2, main="Plot of Observations with Clustering")
lines(x1[1:3], x2[1:3], col="red")
lines(x1[4:6], x2[4:6], col="blue")
legend(0, 200, c('1', '2'))
```
plot(x1,x2, main="Plot of Observations with Clustering")
lines(x1[1:3], x2[1:3], col="red")
lines(x1[4:6], x2[4:6], col="blue")
legend(0, 20, c('1', '2'))
? legend
legend(c(0,20), c('1', '2'))
plot(x1,x2, main="Plot of Observations with Clustering")
lines(x1[1:3], x2[1:3], col="red")
lines(x1[4:6], x2[4:6], col="blue")
legend(c(0,20), c('1', '2'))
plot(x1,x2, main="Plot of Observations with Clustering")
lines(x1[1:3], x2[1:3], col="red")
lines(x1[4:6], x2[4:6], col="blue")
legend(c(0,0), c('1', '2'))
legend(c(0,5), c('1', '2'))
legend(c(5,5), c('1', '2'))
legend(c(5,2), c('1', '2'))
legend(c(2,2), c('1', '2'))
plot(x1,x2, main="Plot of Observations with Clustering")
lines(x1[1:3], x2[1:3], col="red")
lines(x1[4:6], x2[4:6], col="blue")
legend(c(2,2), c('1', '2'))
legend(c(2,2), c('1', '2'), col='red', col='blue)
plot(x1,x2, main="Plot of Observations with Clustering")
lines(x1[1:3], x2[1:3], col="red")
lines(x1[4:6], x2[4:6], col="blue")
legend(c(2,2), c('1', '2'), col='red', col='blue)
```
plot(x1,x2, main="Plot of Observations with Clustering")
lines(x1[1:3], x2[1:3], col="red")
lines(x1[4:6], x2[4:6], col="blue")
legend(c(2,2), c('1', '2'), col='red', col='blue)
## Preliminaries ##
library(ElemStatLearn)
data(nci)
data <- nci
## Section A ##
set.seed(1)
kc1 <- kmeans(data, centers=14)
kc2 <- kmeans(data, centers=14)
kc3 <- kmeans(data, centers=14)
## Section B ##
# Part i
# Lump Error
lump_error <- function(classes, clusters) {
# inputs: arg1: vector of classes
#         arg2: vector of clusters
# outputs: number of lumping errors
#
# usage: lump_error(c(1,2,2), c(2,1,1))
#
count <- 0
for (i in 1:length(clusters)) {
# loop through all clusters
for (y in i:length(clusters)) {
# check if in same cluster
if (clusters[i] == clusters[y]) {
# check if same class
if (classes[i] != classes[y]) {
# if same class, add lump error
count <- count + 1
}
}
}
}
return(count)
}
# test lump_errors function
0 == lump_error(c(1,2,2), c(2,1,1)) # true
2 == lump_error(c(1,2,2), c(1,1,1)) # true
0 == lump_error(c(1,2,2), c(4,5,6)) # true
# Part ii
# Split Error
split_error <- function(classes, clusters) {
# inputs: arg1: vector of classes
#         arg2: vector of clusters
# outputs: number of splitting errors
#
# usage: split_error(c(1,2,2), c(2,1,1))
#
count <- 0
for (i in 1:length(classes)) {
# loop through all classess
for (y in i:length(classes)) {
# check if in same class
if (classes[i] == classes[y]) {
# check if same cluster
if (clusters[i] != clusters[y]) {
# if same cluster, add split error
count <- count + 1
}
}
}
}
return(count)
}
# test split_error function
1 == split_error(c(1,2,2), c(2,1,2)) # true
# Part iii
lump_error()
## Section C ##
split_error(c(1,2,2), c(2,1,1))
split_error(c(1,2,2), c(1,1,1))
kc1 <- kmeans(data, centers=14)
kc1
names(kc1)
c(1,2)
-c(1,2)
str(data)
names(data)
library(ElemStatLearn)
data(nci)
data <- nci
str(dta)
str(data)
names(data)
data
library(ElemStatLearn)
data(nci)
data <- nci
data
names(data)
head(data)
data$dimnames
head(data)
cars
cars$car
cars$cars
colnames(data)
data$CNS
nci
head(nci)
nci$CNR
plot(colnames(data))
colnames(data)
means <- apply(data, 2, mean)
stds <- apply(data, 2, sd)
data.us <- scale(data, center=means, scale=sds)
means <- apply(data, 2, mean)
sds <- apply(data, 2, sd)
data.use <- scale(data, center=means, scale=sds)
data.dist <- dist(data.use)
names(data.dist)
str(data.dist)
colnames(data.dist)
size(data.dist)
Size(data.dist)
length(data.dist)
data.use
names(data.use)
str(data.use)
colnames(data.use)
data.use$CNS
t(data)
data.trans <- t(data)
data.trans
means <- apply(data.trans, 2, mean)
means
sds <- apply(data.trans, 2, sd)
data.use <- scale(data.trans, center=means, scale=sds)
data.dist <- dist(data.use)
? rev
? diff
## Preliminaries ##
library(ElemStatLearn)
data(nci)
data <- nci
## Section A ##
set.seed(1)
kc1 <- kmeans(data, centers=14)
kc2 <- kmeans(data, centers=14)
kc3 <- kmeans(data, centers=14)
for (i in 1:30) {
print(i)
kc <- kmeans(data, centers=i)
}
summary(kc1)
for (i in 1:30) {
print(i)
kc <- kmeans(data, centers=i)
summary(kc)
}
for (i in 1:30) {
print(i)
kc <- kmeans(data, centers=i)
print(summary(kc))
}
for (i in 1:30) {
print('\n')
print(i)
kc <- kmeans(data, centers=i)
print(summary(kc))
}
for (i in 1:30) {
print("\n")
print(i)
kc <- kmeans(data, centers=i)
print(summary(kc))
}
for (i in 1:30) {
cat("\n")
print(i)
kc <- kmeans(data, centers=i)
print(summary(kc))
}
? kmeans
kmeans(t(data), centers=3)
summary(kmeans(t(data), centers=3))
plot(data, kc1)
plot(data, kc1$cluster)
plot(data, kc1$cluster+1)
for (i in 1:30) {
cat("\n")
print(i)
kc <- kmeans(data, centers=i)
print(summary(kc))
}
kc1$tot.withinss
## Section C ##
lines(i, kc$tot.withinss)
}
for (i in 1:30) {
cat("\n")
print(i)
kc <- kmeans(data, centers=i)
print(summary(kc))
lines(i, kc$tot.withinss)
}
plot(0,0)
for (i in 1:30) {
cat("\n")
print(i)
kc <- kmeans(data, centers=i)
print(summary(kc))
lines(i, kc$tot.withinss)
}
for (i in 1:30) {
cat("\n")
print(i)
kc <- kmeans(data, centers=i)
print(summary(kc))
plot(i, kc$tot.withinss)
}
? push
x <- 1:30
x
kc_list <- c()
kc_lits
kc_list
zed <- c()
zed <- c(1,zed)
zed
zed <- c(2,zed)
zed
zed <- c(zed,3)
zed <- c(zed,3)
zed
x <- 1:30
kc_list <- c()
for (i in 1:30) {
cat("\n")
print(i)
kc <- kmeans(data, centers=i)
print(summary(kc))
kc_list <- c(kc_list, kc$tot.withinss)
}
plot(x, kc_list)
x <- 1:30
kc_list <- c()
for (i in 1:30) {
# cat("\n")
# print(i)
kc <- kmeans(data, centers=i)
# print(summary(kc))
kc_list <- c(kc_list, kc$tot.withinss)
}
plot(x, kc_list, main="SSE against Number of Clusters")
sil <- 0
dis.mat <- dist(t(data))
for(k in 2:30) {
print(k)
cluster <- kmeans(t(data), k, nstart=25)$cluster
s <- silhouette(cluster, dis.mat)
ss <- summary(s)
sil[k] <- ss$avg.width
}
plot(1:30, sil, type="l", xlab="Number of Clusters")
for(k in 2:30) {
print(k)
cluster <- kmeans(t(data), k, nstart=25)$cluster
s <- silhouette(cluster, dis.mat)
ss <- summary(s)
sil[k] <- ss$avg.width
}
library(cluster)
sil <- 0
dis.mat <- dist(t(data))
for(k in 2:30) {
print(k)
cluster <- kmeans(t(data), k, nstart=25)$cluster
s <- silhouette(cluster, dis.mat)
ss <- summary(s)
sil[k] <- ss$avg.width
}
plot(1:30, sil, type="l", xlab="Number of Clusters")
hist(x, kc_list)
hist(kc_list)
kc1
summary(kc1)
x = c(1, 2, 3, 4)
x
x[]0
x[1]
x[1:]
x[1,]
x[1,end]
x[1,-1]
x[1,2]
x[1:]
x[1:x.length]
x[1:len(x)]
x[1:9]
names(x)
str(x)
x
help c
x.len
length(x)
x[1:length(x)]
auto.data <- read.csv('C:/Users/Graham/Desktop/FinalProject131/data/data_edited.csv',
header=F)
auto.data
colnames(auto.data) = c(
'symboling',
'normalized.losses',
'make',
'fuel.type',
'aspiration',
'num.doors',
'body.style',
'drive.wheels',
'engine.location',
'wheel.base',
'length',
'width',
'height',
'curb.weight',
'engine.type',
'num.of.cylinders',
'engine.size',
'fuel.system',
'bore',
'stroke',
'compression.ratio',
'horsepower',
'peak.rpm',
'city.mpg',
'highway.mpg',
'price'
)
head(auto.data)
setwd('C:/Users/Graham/Desktop/FinalProject131/src')
source('data.R', local=TRUE)
head(auto.data)
dim(auto.data)
safety.data <- subset(auto.data, select = -c(make))
dim(safety.data)
responseY <- as.matrix(safety.data[,1])
predictorX <- as.matrix(safety.data[, 2:(dim(safety.data)[2])])
pca <- princomp(predictorX, cor=T)
safety.data
head(safety.data)
safety.data
responseX
responseY
predictorX
responseY <- as.matrix(safety.data[,1])
predictorX <- as.matrix(safety.data[, 2:(dim(safety.data)[2])])
# Principal Component Analysis using Correlation matrix
pca <- princomp(predictorX, cor=T)
data.frame
? data.frame
data.frame(safety.data[, 2:(dim(saftey.data)[2])])
safety.data <- subset(auto.data, select = -c(make))
data.frame(safety.data[, 2:(dim(safety.data)[2])])
predictorX <- data.frame(safety.data[, 2:(dim(safety.data)[2])])
head(predictorX)
pca <- princomp(predictorX, cor=T)
data(predictorX)
setwd('C:/Users/Graham/Desktop/FinalProject131/src')
source('data.R', local=TRUE)
head(auto.data)
dim(auto.data)
# Remove non-numeric variables from the dataset
safety.data <- subset(auto.data, select = -c(make))
dim(safety.data)
# Remove unknown observations from the data
# Assign Response and Predictor variables
responseY <- as.matrix(safety.data[,1])
#predictorX <- as.matrix(safety.data[, 2:(dim(safety.data)[2])])
predictorX <- data.frame(safety.data[, 2:(dim(safety.data)[2])])
# Principal Component Analysis using Correlation matrix
pca <- princomp(predictorX, cor=T)
? imputePCA
library(missMDA)
install.packages(missMDA)
install.packages('missMDA')
imputePCA(predictorX)
library(missMDA)
imputePCA(predictorX)
new.data <- na.omit(safety.data)
dim(new.data)
responseY <- as.matrix(new.data[,1])
#predictorX <- as.matrix(safety.data[, 2:(dim(safety.data)[2])])
pca <- princomp(predictorX, cor=T)
head(new.data)
new.data
pca <- princomp(predictorX, cor=T)
pca <- princomp(~ ., data=safety.data, cor = TRUE)
pca <- princomp(~ ., data=predictorX, cor = TRUE)
dim(new.data)
head(new.dta)
head(new.data)
symboling
new.data$symboling
symboling.binary <- ifelse(new.data$symboling <= 0, c('safe'), c('risky') )
symboling.binary
new.data <- subset(new.data, select = -c(symboling))
head(new.data)
