}
plot(1:30, sil, type="l", xlab="Number of Clusters")
hist(x, kc_list)
hist(kc_list)
kc1
summary(kc1)
x = c(1, 2, 3, 4)
x
x[]0
x[1]
x[1:]
x[1,]
x[1,end]
x[1,-1]
x[1,2]
x[1:]
x[1:x.length]
x[1:len(x)]
x[1:9]
names(x)
str(x)
x
help c
x.len
length(x)
x[1:length(x)]
setwd('C:/Users/Graham/Desktop/FinalProject131/src')
source('data.R', local=TRUE)
head(auto.data)
dim(auto.data)
safety.data <- subset(auto.data, select = -c(make))
dim(safety.data)
new.data <- na.omit(safety.data) # remove all NA obs.
dim(new.data)
symboling.binary <- ifelse(new.data$symboling <= 0, c('safe'), c('risky') )
new.data <- subset(new.data, select = -c(symboling))
index <- sample(dim(new.data)[1], size=floor(dim(new.data)[1])*.75,
)
index <- sample(dim(new.data)[1], size=floor(dim(new.data)[1])*.75)
index
index.test <- setdiff(1:dim(safety.data)[1], index)
index.test
length(index)
length(index.test)
index.test <- setdiff(1:dim(new.data)[1], index)
length(index.test)
length(index) + length(index.test)
train.set <- new.data[index,]
dim(train.set)
test.set <- new.data[index.test,]
dim(test.set)
head(new.data)
setwd('C:/Users/Graham/Desktop/FinalProject131/src')
source('prelims.R', local=TRUE)
dim(train.set)
dim(train.set)
dim(test.set)
dim(new.data)
dim(train.set) + dim(test.set)
biSym <- ifelse(new.data$symboling <= 0, c('safe'), c('risky') )
# Remove symboling variable from data
new.data <- subset(new.data, select = -c(symboling))
# Training and Test sets
#########################
# Training set, sample 75% of the data
index <- sample(dim(new.data)[1], size=floor(dim(new.data)[1])*.75)
# Take remaining for test set
biSym <- ifelse(new.data$symboling <= 0, c('safe'), c('risky') )
# Remove symboling variable from data
new.data <- subset(new.data, select = -c(symboling))
head(new.data)
setwd('C:/Users/Graham/Desktop/FinalProject131/src')
source('prelims.R', local=TRUE)
dim(train.set) + dim(test.set)
dim(new.data)
tree.data <- data.frame(new.data, biSym)
dim(tree.data)
head(tree.data)
str(tree.data)
car.tree <- tree(biSym ~., data=tree.data)
library(tree)
car.tree <- tree(biSym ~., data=tree.data)
car.tree
summary(car.tree)
plot(car.tree)
text(car.tree, pretty=0, cex=1, col="red")
text(car.tree, pretty=0, cex=0.8, col="red")
plot(car.tree)
text(car.tree, pretty=0, cex=0.8, col="red")
title('Classification Tree')
car.tree <- tree(biSym ~., data=tree.data, subset=train)
car.tree <- tree(biSym ~., data=tree.data, subset=train.set)
length(index)
car.tree <- tree(biSym ~., data=tree.data, subset=index)
tree.pred <- predict(car.tree, test.set, type='class')
tree.pred
tree.pred <- predict(car.tree, train.set, type='class')
error <- table(tree.pred, test.set)
index <- sample(dim(new.data)[1], size=floor(dim(new.data)[1])*.5)
index.test <- setdiff(1:dim(new.data)[1], index)
length(index) + length(index.test) # 159, equal to number of obs.
# Our two sets:
train.set <- new.data[index,]
dim(train.set)
test.set <- new.data[index.test,]
dim(test.set)
car.tree <- tree(biSym ~., data=tree.data, subset=index)
# Predict on test set
tree.pred <- predict(car.tree, train.set, type='class')
# Confusion matrix
error <- table(tree.pred, test.set)
# Classification Tree using Random Forest
cv.car <- cv.tree(car.tree, FUN=prune.tree, k=10, method='misclass')
cv.car <- cv.tree(car.tree, FUN=prune.tree, K=10, method='misclass')
class(cv.car)
names(cv.car)
cv.car
plot(cv.car$size, cv.car$dev, type='b',
xlab='Number of Nodes',
ylab='CV Misclassification Error',
col='red')
plot(cv.car$k, cv.car$dev, type='b',
xlab='Number of Nodes',
ylab='CV Misclassification Error',
col='red')
plot(cv.car$k, cv.car$dev, type='b',
xlab='Complexity',
ylab='',
col='blue')
plot(cv.car$size, cv.car$dev, type='b',
xlab='Number of Nodes',
ylab='CV Misclassification Error',
col='red')
abline(v = cv.car$k[min.error], lty=2)
min.error <- which.min(cv.car$dev) # index of min. error
plot(cv.car$size, cv.car$dev, type='b',
xlab='Number of Nodes',
ylab='CV Misclassification Error',
col='red')
min.error <- which.min(cv.car$dev) # index of min. error
abline(h = cv.car$dev[min.error], lty=2)
abline(v = cv.car$k[min.error], lty=2)
plot(cv.car$k, cv.car$dev, type='b',
xlab='Complexity',
ylab='CV Misclassification Error',
col='blue')
# Get minimum error
min.error <- which.min(cv.car$dev) # index of min. error
abline(h = cv.car$dev[min.error], lty=2)
abline(v = cv.car$k[min.error], lty=2)
abline(v = cv.car$k[min.error], lty=2)
prune.car <- prune.misclass(car.tree, best=5)
plot(prune.car)
text(prune.car, pretty=0, col='red', cex=0.8)
title('Pruned Tree')
min.error <- which.min(cv.car$dev) # index of min. error
plot(cv.car$k, cv.car$dev, type='b',
xlab='Complexity',
ylab='CV Misclassification Error',
col='blue')
# Get minimum error
min.error <- which.min(cv.car$dev) # index of min. error
abline(h = cv.car$dev[min.error], lty=2)
abline(v = cv.car$k[min.error], lty=2)
tree.pred <- predict(car.tree, train.set, type='class')
summary(tree.pred)
error <- table(tree.pred, test.set)
index.test <- setdiff(1:dim(new.data)[1], index)
index <- sample(dim(new.data)[1], size=floor(dim(new.data)[1])*.50)
# Take remaining for test set
index.test <- setdiff(1:dim(new.data)[1], index)
length(index) + length(index.test) # 159, equal to number of obs.
length(index)
length(index.test)
head(index.test)
index.test
index.test <- c(index.test, -1)
index.test
index.test <- c(index.test, select=c(-1))
index.test
index.test <- setdiff(1:dim(new.data)[1], index)
index.test <- c(index.test, select=c(-1))
index.test
index.test <- setdiff(1:dim(new.data)[1], index)
index.test <- subset(index.test, select=c(-1))
index <- sample(dim(new.data)[1], size=floor(dim(new.data)[1])*.50)
# Take remaining for test set
index.test <- setdiff(1:dim(new.data)[1], index)
car.tree <- tree(biSym ~., data=tree.data, subset=index)
# Predict on test set
tree.pred <- predict(car.tree, train.set, type='class')
# Confusion matrix
error <- table(tree.pred, test.set)
# Classification Error
1 - sum(diag(error))/sum(error)
# K-fold Cross Validation
##########################
# Note that Cross Validation automatically declares training and test sets
cv.car <- cv.tree(car.tree, FUN=prune.tree, K=10, method='misclass')
cv.car
# Plot
# Misclassification error vs. number nodes
plot(cv.car$size, cv.car$dev, type='b',
xlab='Number of Nodes',
ylab='CV Misclassification Error',
col='red')
# Complexity and Min. Error
plot(cv.car$k, cv.car$dev, type='b',
xlab='Complexity',
ylab='CV Misclassification Error',
col='blue')
# Get minimum error
min.error <- which.min(cv.car$dev) # index of min. error
abline(h = cv.car$dev[min.error], lty=2)
abline(v = cv.car$k[min.error], lty=2)
# Prune tree
prune.car <- prune.misclass(car.tree, best=5)
plot(prune.car)
text(prune.car, pretty=0, col='red', cex=0.8)
title('Pruned Tree')
plot(cv.car$k, cv.car$dev, type='b',
xlab='Complexity',
ylab='CV Misclassification Error',
col='blue')
# Get minimum error
min.error <- which.min(cv.car$dev) # index of min. error
abline(h = cv.car$dev[min.error], lty=2)
abline(v = cv.car$k[min.error], lty=2)
# Training set, sample 75% of the data
index <- sample(dim(new.data)[1], size=floor(dim(new.data)[1])*.75)
# Take remaining for test set
index.test <- setdiff(1:dim(new.data)[1], index)
length(index) + length(index.test) # 159, equal to number of obs.
# Our two sets:
train.set <- new.data[index,]
dim(train.set)
test.set <- new.data[index.test,]
dim(test.set)
set.seed(2)
# Use training set and test set to evaluate our classification tree
car.tree <- tree(biSym ~., data=tree.data, subset=index)
# Predict on test set
tree.pred <- predict(car.tree, train.set, type='class')
# Confusion matrix
error <- table(tree.pred, test.set)
# Note that Cross Validation automatically declares training and test sets
cv.car <- cv.tree(car.tree, FUN=prune.tree, K=10, method='misclass')
cv.car
plot(cv.car$size, cv.car$dev, type='b',
xlab='Number of Nodes',
ylab='CV Misclassification Error',
col='red')
# Complexity and Min. Error
plot(cv.car$k, cv.car$dev, type='b',
xlab='Complexity',
ylab='CV Misclassification Error',
col='blue')
# Get minimum error
min.error <- which.min(cv.car$dev) # index of min. error
abline(h = cv.car$dev[min.error], lty=2)
abline(v = cv.car$k[min.error], lty=2)
# Prune tree
prune.car <- prune.misclass(car.tree, best=5)
plot(prune.car)
text(prune.car, pretty=0, col='red', cex=0.8)
title('Pruned Tree')
summary(prune.car)
set.seed(2)
# Use training set and test set to evaluate our classification tree
car.tree <- tree(biSym ~., data=tree.data, subset=index)
# Predict on test set
tree.pred <- predict(car.tree, train.set, type='class')
# Confusion matrix
error <- table(tree.pred, test.set)
cv.car <- cv.tree(car.tree, FUN=prune.tree, K=10, method='misclass')
cv.car
plot(cv.car$size, cv.car$dev, type='b',
xlab='Number of Nodes',
ylab='CV Misclassification Error',
col='red')
# Complexity and Min. Error
plot(cv.car$k, cv.car$dev, type='b',
xlab='Complexity',
ylab='CV Misclassification Error',
col='blue')
prune.car <- prune.misclass(car.tree, best=6)
plot(prune.car)
text(prune.car, pretty=0, col='red', cex=0.8)
title('Pruned Tree')
summary(prune.car)
table(biSym, predict(car.tree, train.set, type='class'))
length(biSym)
length(tree.pred)
table(train.set$biSym, predict(car.tree, train.set, type='class'))
head(train.set)
set.seed(2)
# Use training set and test set to evaluate our classification tree
car.tree <- tree(biSym ~., data=tree.data, subset=index)
# Predict on test set
tree.pred <- predict(car.tree, train.set, type='class')
length(tree.pred)
length(test.set)
length(train.set)
# Training set, sample 75% of the data
index <- sample(dim(new.data)[1], size=floor(dim(new.data)[1])*.75)
# Take remaining for test set
index.test <- setdiff(1:dim(new.data)[1], index)
length(index) + length(index.test) # 159, equal to number of obs.
# Our two sets:
train.set <- new.data[index,]
dim(train.set)
test.set <- new.data[index.test,]
dim(test.set)
length(train.set)
dim(train.set)
dim(tree.pred)
length(tree.pred)
set.seed(2)
# Use training set and test set to evaluate our classification tree
car.tree <- tree(biSym ~., data=tree.data, subset=index)
cv.car <- cv.tree(car.tree, FUN=prune.tree, K=10, method='misclass')
cv.car
# Plot
# Misclassification error vs. number nodes
plot(cv.car$size, cv.car$dev, type='b',
xlab='Number of Nodes',
ylab='CV Misclassification Error',
col='red')
plot(cv.car$k, cv.car$dev, type='b',
xlab='Complexity',
ylab='CV Misclassification Error',
col='blue')
# Get minimum error
set.seed(2)
car.tree <- tree(biSym ~., data=tree.data, subset=index)
cv.car <- cv.tree(car.tree, FUN=prune.tree, K=10, method='misclass')
cv.car
plot(cv.car$size, cv.car$dev, type='b',
xlab='Number of Nodes',
ylab='CV Misclassification Error',
col='red')
plot(cv.car$k, cv.car$dev, type='b',
xlab='Complexity',
ylab='CV Misclassification Error',
col='blue')
set.seed(1)
# Use training set and test set to evaluate our classification tree
car.tree <- tree(biSym ~., data=tree.data, subset=index)
cv.car <- cv.tree(car.tree, FUN=prune.tree, K=10, method='misclass')
cv.car
plot(cv.car$size, cv.car$dev, type='b',
xlab='Number of Nodes',
ylab='CV Misclassification Error',
col='red')
plot(cv.car$k, cv.car$dev, type='b',
xlab='Complexity',
ylab='CV Misclassification Error',
col='blue')
min.error <- which.min(cv.car$dev) # index of min. error
abline(h = cv.car$dev[min.error], lty=2)
abline(v = cv.car$k[min.error], lty=2)
prune.car <- prune.misclass(car.tree, best=6)
plot(prune.car)
text(prune.car, pretty=0, col='red', cex=0.8)
title('Pruned Tree')
summary(prune.car)
tree.pred <- predict(car.tree, test.set, type='class')
error <- table(tree.pred, test.set)
set.seed(1)
index <- sample(dim(new.data)[1], size=floor(dim(new.data)[1])*.75)
# Take remaining for test set
index.test <- setdiff(1:dim(new.data)[1], index)
length(index) + length(index.test) # 159, equal to number of obs.
# Our two sets:
train.set <- new.data[index,]
dim(train.set)
test.set <- new.data[index.test,]
dim(test.set)
# Use training set and test set to evaluate our classification tree
car.tree <- tree(biSym ~., data=tree.data, subset=index)
cv.car <- cv.tree(car.tree, FUN=prune.tree, K=10, method='misclass')
cv.car
plot(cv.car$size, cv.car$dev, type='b',
xlab='Number of Nodes',
ylab='CV Misclassification Error',
col='red')
# Complexity and Min. Error
plot(cv.car$k, cv.car$dev, type='b',
xlab='Complexity',
ylab='CV Misclassification Error',
col='blue')
# Get minimum error
min.error <- which.min(cv.car$dev) # index of min. error
abline(h = cv.car$dev[min.error], lty=2)
abline(v = cv.car$k[min.error], lty=2)
tree.pred <- predict(car.tree, test.set, type='class')
# Confusion matrix
error <- table(tree.pred, test.set)
car.tree <- tree(biSym ~., data=tree.data, subset=index)
set.seed(1)
index <- sample(dim(new.data)[1], size=floor(dim(new.data)[1])*.75)
# Take remaining for test set
index.test <- setdiff(1:dim(new.data)[1], index)
length(index) + length(index.test) # 159, equal to number of obs.
train.set <- new.data[index,]
dim(train.set)
test.set <- new.data[index.test,]
dim(test.set)
car.tree <- tree(biSym ~., data=tree.data, subset=index)
cv.car <- cv.tree(car.tree, FUN=prune.tree, K=10, method='misclass')
cv.car
plot(cv.car$size, cv.car$dev, type='b',
xlab='Number of Nodes',
ylab='CV Misclassification Error',
col='red')
tree.pred <- predict(car.tree, test.set, type='class')
error <- table(tree.pred, test.set)
length(tree.pred)
length(test.set)
symbol.test <- biSym[index.test,]
symbol.test <- biSym[index.test]
symbol.test
error <- table(tree.pred, symbol.test)
1 - sum(diag(error))/sum(error)
library(randomForest)
install.packages(randomforest)
install.packages(randomForest)
install.packages('randomForest')
fit <- randomForest(biSym ~., data=tree.data)
library(randomForest)
fit <- randomForest(biSym ~., data=tree.data)
print(fit)
importance(fit)
importance(fit)
plot.randomForest(fit)
plot(fit)
importance(fit)
plot(cv.car$size, cv.car$dev, type='b',
xlab='Number of Nodes',
ylab='CV Misclassification Error',
col='red')
prune.car <- prune.misclass(car.tree, best=6)
plot(prune.car)
text(prune.car, pretty=0, col='red', cex=0.8)
View(new.data)
library(tree)
library(randomForest)
# load data from 'prelims.R'
setwd('C:/Users/Graham/Desktop/FinalProject131/src')
source('prelims.R', local=TRUE)
dim(train.set) + dim(test.set)
dim(new.data)
tree.data <- data.frame(new.data, biSym)
car.tree <- tree(biSym ~., data=tree.data)
summary(car.tree)
# Plot the tree and labels
plot(car.tree)
text(car.tree, pretty=0, cex=0.8, col='red')
title('Classification Tree')
set.seed(1)
# Training and Test sets
# Training set, sample 75% of the data
index <- sample(dim(new.data)[1], size=floor(dim(new.data)[1])*.75)
# Take remaining for test set
index.test <- setdiff(1:dim(new.data)[1], index)
length(index) + length(index.test) # 159, equal to number of obs.
train.set <- new.data[index,]
dim(train.set)
symbol.test <- biSym[index.test]
test.set <- new.data[index.test,]
dim(test.set)
# Model Validation
# Use training set and test set to evaluate our classification tree
car.tree <- tree(biSym ~., data=tree.data, subset=index)
# Predict on test set
tree.pred <- predict(car.tree, test.set, type='class')
# Confusion matrix
error <- table(tree.pred, symbol.test)
# Classification Error
1 - sum(diag(error))/sum(error)
# K-fold Cross Validation
##########################
# Note that Cross Validation automatically declares training and test sets
cv.car <- cv.tree(car.tree, FUN=prune.tree, K=10, method='misclass')
cv.car
# Plot
# Misclassification error vs. number nodes
plot(cv.car$size, cv.car$dev, type='b',
xlab='Number of Nodes',
ylab='CV Misclassification Error',
col='red')
# Complexity and Min. Error
plot(cv.car$k, cv.car$dev, type='b',
xlab='Complexity',
ylab='CV Misclassification Error',
col='blue')
# Get minimum error
min.error <- which.min(cv.car$dev) # index of min. error
abline(h = cv.car$dev[min.error], lty=2)
abline(v = cv.car$k[min.error], lty=2)
# Prune tree
prune.car <- prune.misclass(car.tree, best=6)
plot(prune.car)
text(prune.car, pretty=0, col='red', cex=0.8)
prune.car <- prune.misclass(car.tree, best=8)
plot(prune.car)
text(prune.car, pretty=0, col='red', cex=0.8)
title('Pruned Tree')
# Classification Tree using Random Forest
##########################################
fit <- randomForest(biSym ~., data=tree.data)
print(fit)
