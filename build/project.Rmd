---
title: "Safety Analysis of Automobiles"
author: "Graham Marlow"
date: "June 8, 2015"
output: html_document
---
# Abstract


# Introduction

- - -

# Data Description

The data that I used for this project comes from the UCI Machine Learning Repository. The dataset is called `automobile` and contains `25` variables, the most important of which is `symboling`, the index of safety that this project will use to classify the data.

```{r, echo=FALSE}
setwd('C:/Users/Graham/Desktop/FinalProject131/src')
source('prelims.R', local=TRUE)
data <- data.frame(new.data, num.binary)

# Show distribution of symboling, vanilla data
par(mfrow=c(1, 2))
hist(safety.data$symboling,
     xlab = 'Symboling',
     ylab = 'Frequency',
     main = 'Histogram of Symboling')

hist(data$symboling,
     xlab = 'Symboling',
     ylab = 'Frequency',
     main = 'Histogram of Symboling, Partial Data')
```

Left is a histogram of `symboling` from the vanilla dataset. As we can see from the distribution of frequencies, most of the observations are either `0` or positive, with only a very few vehicles being classified as completley safe in the negative values. Therefore, to balance the data for my analysis, I will count a `symboling` value of `0` as 'safe'.

Additionally, due to the fact that the random forest method will not work with any blank or `NA` values, I removed all observations that contained such values.

The right histogram is the new distribution of `symboling` values, in a dataset with  all observations containing `NA` removed. There is not a whole lot of change happening between the two distributions, so I think that removing the observations that contain blank values is justified.


```{r, echo=FALSE}
# Distribution of Binary variable
par(mfrow=c(1, 1))
hist(data$num.binary,
     xlab = 'Symboling',
     ylab = 'Frequency',
     main = 'Histogram of Risky(0) and Safe (1)')
```

Here is the histogram of the data with a a new binary variable, where "risky" autos are depicted as a `0` and "safe" autos are a `1`. This provides more insight into how the data is handled when I am forming trees, as I decide to use a binary variable over the vanilla symboling variable. To create this binary variable, I grouped all autos with a symboling `<= 0` as "safe", and all autos with symboling `> 0` as "risky".

This new binary variable has more risky variables than safe variables, just as the other histograms demonstrate. However, because `0` is declared as "safe", there is a better balance between the two groupings.

### Regression

As an initial test of the data, I created a linear regression model around the variable that I thought contributed most to `symboling`, `price`.

```{r}
# Simple linear regression, price vs. safety
fit.price <- lm(data$num.binary ~ data$price)
summary(fit.price)
```

Since there is a signicantly significant relationship between `price` and `symboling` (in terms of the slope of the regression line), there is a strong possibility that `price` is a strong contributing factor to `symboling`. I will test this hypothesis later in the project to see if my prediction holds up to the actual classification.



# Tree-based Methods

In this section, I will be performing a number of different classification trees using different classification methods.

### Binary Recursive Partitioning

The first technique I will use is binary recursive partitioning. This is the most basic method out of the four that I will cover, and therefore results in the greatest misclassification error.

```{r}
# Load required packages
library(tree)
library(randomForest)
library(ada)

# Attach binary variable to dataset, remove symboling
new.data <- subset(new.data, select = -c(symboling))
tree.data <- data.frame(new.data, biSym)

# Simple Classification Tree, no Model Evaluation
##################################################
# Generate tree using defaults
car.tree <- tree(biSym ~., data=tree.data)
summary(car.tree)

# Plot the tree and labels
par(mfrow=c(1, 1))
plot(car.tree)
text(car.tree, pretty=0, cex=0.8, col='red')
title('Classification Tree, Binary Recursive Partitioning')
```

The binary recursive partitioning method involves partitioning the response variable (`symboling`) and choosing splits from the rest of the variables in the dataset. Splits are chosen based off of their impurity -- we wish to maximize the reduction in impurity based off of the variables. The splitting halts when the terminal nodes are too small or too few to be split.

The classification tree in this case has 10 nodes, detailing these `7` variables: number of doors, bore, normalized losses, fuel system, wheel base, engine size, and stroke. Number of doors seems to be the most important variable, as it is the root of the tree. If the number of doors of a given car is *not* four, the car is very likely to be risky.

Next, I want to use model validation techniques to test the previous modelling technique and see whether or not we can prune it. This is accomplished through the use of a training set and a test set. I will use the training set, consisting of `75%` of the data, to model the data and a test set, consisting of the remaining data, to test the model.

```{r}
# Model Validation
###################
set.seed(1)
# Training and Test sets
# Training set, sample 75% of the data
index <- sample(dim(new.data)[1], size=floor(dim(new.data)[1])*.75)
# Take remaining for test set
index.test <- setdiff(1:dim(new.data)[1], index)

# Our two sets:
train.set <- new.data[index,]

symbol.test <- biSym[index.test]
test.set <- new.data[index.test,]

# Model Validation
# Use training set and test set to evaluate our classification tree
car.tree <- tree(biSym ~., data=tree.data, subset=index)

# Predict on test set
tree.pred <- predict(car.tree, test.set, type='class')

# Confusion matrix
error <- table(tree.pred, symbol.test)

# Classification Error
1 - sum(diag(error))/sum(error)
```

In the above code, I separated the original dataset into a training set and test set, modeled a tree from the training set, and compared it to a tree that is predicted based off of the test set. Error is calculated based off of comparing the predicted model (from the test set) to the response variable (in our case, `symboling`.)

After validating the model using a training set and test set, we are left with a misclassification error of `10%`. Now, this isn't a horrible amount of error, but I think we can do better.

### K-fold Cross Validation

The second technique that I will use to create a classification tree is k-fold cross validation. This method will automatically create a training set and test set, based off of our original data. In effect, the process of training a tree and testing it will be repeated multiple times on slightly different subsets of our automobile dataset. The advantage to using this technique over recursive partitioning is reduced variability, due to the fact that we are predicting the fit of a model based off of a hypothetitical validation set.

However, there is also a downside to using cross validation: one must declare a parameter `K` to judge their prediction. `K` is the number of folds that take place in the cross validation, and my choice of `K = 10` is fairly arbitrary, as it seems to be a good spot between having a large number of sample combinations (low `K`), and a low number of sample combinations (high `K`).

```{r, echo=FALSE}
# K-fold Cross Validation
##########################
# Note that Cross Validation automatically declares training and test sets
cv.car <- cv.tree(car.tree, FUN=prune.tree, K=10, method='misclass')

# Plot
# Misclassification error vs. number nodes
par(mfrow=c(2, 1))
plot(cv.car$size, cv.car$dev, type='b', 
     xlab='Number of Nodes',
     ylab='CV Misclassification Error',
     col='red')
title('Cross Validation Exploratory Analysis')
# Complexity and Min. Error
plot(cv.car$k, cv.car$dev, type='b', 
     xlab='Complexity',
     ylab='CV Misclassification Error',
     col='blue')

# Get minimum error
min.error <- which.min(cv.car$dev) # index of min. error
abline(h = cv.car$dev[min.error], lty=2)
abline(v = cv.car$k[min.error], lty=2)
```

Before continuing with the cross validation technique, it is important to examine an optimal level of `k`, or the number of nodes that the tree should have. Given the first plot, number of nodes vs. error, we can see that we attain the minimum error at `k = 6`. Therefore, we should use `6` nodes when we construct our tree. The importance of reducing the complexity of the model is demonstrated through the complexity vs. error plot, where a minimum error is attained by having the minimum amount of complexity.

``` {r}
# Prune tree
par(mfrow=c(1, 1))
prune.car <- prune.misclass(car.tree, best=6)
plot(prune.car)
text(prune.car, pretty=0, col='red', cex=0.8)
title('Pruned Tree')
summary(prune.car)
```

Given these estimations, we now form a new tree based upon the optimal number of nodes, `6`. This tree cuts out the less important variables from the model, while maintaining the ones that contribute most towards classifying riskiness. This tree demonstrates that all observations that have a number of doors different from four are classified as risky, which is fairly significant. The other important classifying variables include bore, normalized losses, wheel base, and engine size.

The error for this tree is `5.8%`, significantly better than the binary recursive tree.

### Random Forest

The next method that I will use to classify riskiness is the random forest method. This method involves boostrapping a large number of tree simulations, and using the mean result as the final product. This usually results in a better model than single-based tree models.

```{r}
# Classification Tree using Random Forest
##########################################
fit <- randomForest(biSym ~., data=tree.data)
plot(fit,
      main = 'Random Forest Method')
print(fit)
```

The plot above gives an illustration of all of the tree simulations and their respective errors. As the number of simulations increases, the amount of error decreases until it begins to normalize around a specific value, in this case, `5%`. The red dashed line represents the lower bound of the confidience interval around the error, and the green dashed line represents the upper bound.

The end misclassification error of `5%` is better than the cross validation error, and supports the reasoning that simulating many trees gives a better result than using just one. Therefore, our preferred model in this project is the random forest model.


```{r}
# List importance
importance(fit)
```

The above list shows all of the variables of the dataset and their respective importance values. A higher importance indicates the the variable is more useful in classifying whether or not an auto is risky or safe. The results from the random forest method are actually very similar to the results from the cross validation method, where number of doors, wheel base, and normalized losses are the most important variables to consider. However, unlike the cross validation model, our random forest model does not think that engine size is an important factor in determining riskiness. To further test the variables chosen, I will use the boosted method of classification.

### Boosting Method

The boosting method, similar to random forests, uses a bootstrap process to simulate the creation of a large number of trees. However, unlike the random forest method, boosting gradually refits the data on each iteration, effectively improving it. Therefore, each new iteration is dependent on the previous. This makes for a computationally intensive, yet accurate classification.

```{r}
# Classification with Boosting
###############################
fit.boost <- ada(biSym ~., data=tree.data)
fit.boost
```

The misclassification error for the boosting method is `5.7%`. This is not as good as the random forest method, so we will continue using that classification tree. However, it is important to note that the variables that the boosting method declares most important are very different from the random forest method.

```{r}
# Plot variable importance
varplot(fit.boost)
```

In the boosting method, length, price, and weight are determined the three most important variables to classifying the data. This contrasts the other methods, where number of doors and normalized losses are among the most important variables.



# Conclusion

When classifying the automobile dataset by `symboling`, it seems that the random forest method provides the best results. The random forest classification tree resulted in a misclassification error of about `5%`, the lowest of the trees. Furthermore, the random forest method reduces variability due to bootstrapping, making it favorable to cross validation and binary partitioning methods.

In terms of the most important variables, my initial prediction was incorrect. According to the random forest method, the most important variables when classifying riskiness were number of doors, normalized losses, and wheel base. Price ended up to be a rather unimportant variable in the classification process.

To further the results from this project, more analysis could be conducted on what aspects of each of the major variables can be changed in order to yield safe results. For example, because we now know that normalized losses, wheel base, and number of doors are the most important variables when it comes to classifying an auto as safe, we could test what qualities of each of these variables should be changed in order to achieve a smaller risk rating. For example, if one wants to make a family-safe vehicle, the results of this project suggest that the vehicle should have four doors.